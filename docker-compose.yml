version: "3.3"

services:
  namenode:
    image: jpbarddal/hadoop-namenode:1.0
    restart: always
    hostname: namenode
    ports:
      - 50070:9870
      - 50030:50030
    volumes:
      - ./data/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env

  datanode-1:
    hostname: datanode-1
    image: jpbarddal/hadoop-datanode:1.0
    restart: always
    volumes:
      - ./data/datanode1:/hadoop/dfs/data
    env_file:
      - ./hadoop.env
    depends_on:
      - namenode

  datanode-2:
    hostname: datanode-2
    image: jpbarddal/hadoop-datanode:1.0
    restart: always
    volumes:
      - ./data/datanode2:/hadoop/dfs/data
    env_file:
      - ./hadoop.env
    depends_on:
      - namenode

  spark-master:
    image: jpbarddal/spark-master:1.0
    restart: always
    hostname: spark-master
    ports:
      - 8989:8080
      - 7077:7077
    volumes:
      - ./zeppelin/conf/hadoop/core-site.xml:/opt/hadoop-3.1.1/etc/hadoop/core-site.xml:ro
      - ./data/sparkmaster:/hadoop/dfs/data
    env_file:
      - ./spark.env

  spark-worker1:
    hostname: spark-worker1
    image: jpbarddal/spark-worker:1.0
    restart: always
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
    env_file:
      - ./spark.env
    volumes:
      - ./zeppelin/conf/hadoop/core-site.xml:/opt/hadoop-3.1.1/etc/hadoop/core-site.xml:ro
      - ./data/sparkworker1:/hadoop/dfs/data

  spark-worker2:
    hostname: spark-worker2
    image: jpbarddal/spark-worker:1.0
    restart: always
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
    env_file:
      - ./spark.env
    volumes:
      - ./zeppelin/conf/hadoop/core-site.xml:/opt/hadoop-3.1.1/etc/hadoop/core-site.xml:ro
      - ./data/sparkworker2:/hadoop/dfs/data

  spark-gui:
    hostname: spark-gui
    #user: "1000:100"
    image: jupyter/pyspark-notebook:spark-3.2.1
    restart: always
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
    env_file:
      - ./spark.env
    ports:
      - 50080:8888
    depends_on:
      - spark-master
    volumes:
      - ./data/sparkgui:/home/jovyan/work
    user: root
    working_dir: /home/jovyan/work
    environment:
      JUPYTER_ENABLE_LAB: 1
      #NB_USER: ${USER}
      NB_GID: 1000

  historyserver:
    image: jpbarddal/hadoop-historyserver:1.0
    container_name: historyserver
    depends_on:
      - namenode
      - datanode-1
      - datanode-2
    env_file:
      - ./hadoop.env

  nodemanager:
    image: jpbarddal/hadoop-nodemanager:1.0
    container_name: nodemanager1
    depends_on:
      - namenode
      - datanode-1
      - datanode-2
    env_file:
      - ./hadoop.env
    volumes:
      - ./data/nodemanager:/hadoop/dfs/data

  resourcemanager:
    image: jpbarddal/hadoop-resourcemanager:1.0
    hostname: resourcemanager
    ports:
      - 8088:8088
    depends_on:
      - namenode
      - datanode-1
      - datanode-2
    volumes:
      - ./data/resourcemanager:/hadoop/dfs/data
    env_file:
      - ./hadoop.env
    healthcheck:
      disable: true
